"""
æŸ¥è¯¢å‘é‡åŒ–çš„ Sutton RL æ•™ç§‘ä¹¦
ä½¿ç”¨è¯­ä¹‰æœç´¢æ‰¾åˆ°ç›¸å…³å†…å®¹

ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆå®Œå…¨å…è´¹ï¼‰
"""

import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer

# é…ç½®
VECTORS_PATH = Path("../resources/textbook_vectors.json")
EMBEDDING_MODEL = "all-MiniLM-L6-v2"  # ä¸å‘é‡åŒ–æ—¶ä½¿ç”¨çš„æ¨¡å‹ä¸€è‡´


def load_vectors() -> Dict:
    """åŠ è½½å‘é‡æ•°æ®"""
    print(f"ğŸ“‚ åŠ è½½å‘é‡æ•°æ®: {VECTORS_PATH}")
    
    with open(VECTORS_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"âœ“ åŠ è½½äº† {data['metadata']['total_chunks']} ä¸ªæ–‡æœ¬å—")
    return data


def get_query_embedding(query: str) -> List[float]:
    """è·å–æŸ¥è¯¢çš„å‘é‡ï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰"""
    model = SentenceTransformer(EMBEDDING_MODEL)
    embedding = model.encode([query], show_progress_bar=False)
    return embedding[0].tolist()


def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))


def search(query: str, data: Dict, top_k: int = 5) -> List[Tuple[Dict, float]]:
    """æœç´¢æœ€ç›¸å…³çš„æ–‡æœ¬å—"""
    print(f"ğŸ” æœç´¢: '{query}'")
    
    # è·å–æŸ¥è¯¢å‘é‡
    query_embedding = get_query_embedding(query)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    results = []
    for chunk in data['chunks']:
        similarity = cosine_similarity(query_embedding, chunk['embedding'])
        results.append((chunk, similarity))
    
    # æ’åºå¹¶è¿”å› top_k
    results.sort(key=lambda x: x[1], reverse=True)
    return results[:top_k]


def display_results(results: List[Tuple[Dict, float]]):
    """æ˜¾ç¤ºæœç´¢ç»“æœ"""
    print("\n" + "=" * 80)
    print("æœç´¢ç»“æœ")
    print("=" * 80)
    
    for i, (chunk, score) in enumerate(results, 1):
        print(f"\nã€ç»“æœ {i}ã€‘ç›¸ä¼¼åº¦: {score:.4f} | é¡µç : {chunk['page']}")
        print("-" * 80)
        
        # æ˜¾ç¤ºæ–‡æœ¬ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
        text = chunk['text']
        if len(text) > 300:
            text = text[:300] + "..."
        print(text)
        print()


def interactive_mode():
    """äº¤äº’å¼æŸ¥è¯¢æ¨¡å¼"""
    print("\n" + "=" * 80)
    print("Sutton RL æ•™ç§‘ä¹¦ - äº¤äº’å¼æŸ¥è¯¢")
    print("=" * 80)
    print("è¾“å…¥æŸ¥è¯¢å†…å®¹ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print()
    
    # åŠ è½½æ•°æ®
    data = load_vectors()
    
    while True:
        try:
            query = input("\nğŸ” æŸ¥è¯¢ > ").strip()
            
            if query.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            if not query:
                continue
            
            # æœç´¢
            results = search(query, data, top_k=3)
            display_results(results)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


def single_query(query: str, top_k: int = 5):
    """å•æ¬¡æŸ¥è¯¢"""
    data = load_vectors()
    results = search(query, data, top_k=top_k)
    display_results(results)


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) > 1:
        # å‘½ä»¤è¡ŒæŸ¥è¯¢
        query = ' '.join(sys.argv[1:])
        single_query(query)
    else:
        # äº¤äº’å¼æ¨¡å¼
        interactive_mode()


if __name__ == "__main__":
    main()
