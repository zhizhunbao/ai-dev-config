"""
å‘é‡åŒ– Sutton RL æ•™ç§‘ä¹¦
ç”Ÿæˆå•ä¸ª JSON æ–‡ä»¶å­˜å‚¨æ–‡æœ¬å—å’Œå‘é‡

ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆå®Œå…¨å…è´¹ï¼‰
"""

import json
from pathlib import Path
from typing import List, Dict
import pypdf
from sentence_transformers import SentenceTransformer

# é…ç½®
PDF_PATH = Path("../resources/SuttonReinforcementLearning.pdf")
OUTPUT_PATH = Path("../resources/textbook_vectors.json")
CHUNK_SIZE = 500  # æ¯å—å­—ç¬¦æ•°
CHUNK_OVERLAP = 50  # é‡å å­—ç¬¦æ•°
EMBEDDING_MODEL = "all-MiniLM-L6-v2"  # æœ¬åœ°æ¨¡å‹ï¼Œå®Œå…¨å…è´¹


def extract_text_from_pdf(pdf_path: Path) -> List[Dict[str, any]]:
    """ä» PDF æå–æ–‡æœ¬ï¼ŒæŒ‰é¡µåˆ†ç»„"""
    print(f"ğŸ“– è¯»å– PDF: {pdf_path}")
    
    pages = []
    with open(pdf_path, 'rb') as file:
        reader = pypdf.PdfReader(file)
        total_pages = len(reader.pages)
        
        for page_num, page in enumerate(reader.pages, 1):
            text = page.extract_text()
            if text.strip():
                pages.append({
                    'page': page_num,
                    'text': text.strip()
                })
            
            if page_num % 50 == 0:
                print(f"  å¤„ç†è¿›åº¦: {page_num}/{total_pages} é¡µ")
    
    print(f"âœ“ æå–äº† {len(pages)} é¡µæ–‡æœ¬")
    return pages


def chunk_text(pages: List[Dict], chunk_size: int, overlap: int) -> List[Dict]:
    """å°†æ–‡æœ¬åˆ†å—"""
    print(f"âœ‚ï¸  åˆ†å—æ–‡æœ¬ (å—å¤§å°: {chunk_size}, é‡å : {overlap})")
    
    chunks = []
    chunk_id = 0
    
    for page_data in pages:
        text = page_data['text']
        page_num = page_data['page']
        
        # ç®€å•åˆ†å—ï¼šæŒ‰å­—ç¬¦æ•°
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunk_text = text[start:end]
            
            if chunk_text.strip():
                chunks.append({
                    'id': chunk_id,
                    'page': page_num,
                    'text': chunk_text.strip(),
                    'start': start,
                    'end': end
                })
                chunk_id += 1
            
            start = end - overlap
    
    print(f"âœ“ ç”Ÿæˆäº† {len(chunks)} ä¸ªæ–‡æœ¬å—")
    return chunks


def get_embeddings(texts: List[str]) -> List[List[float]]:
    """ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·å–å‘é‡ï¼ˆå®Œå…¨å…è´¹ï¼‰"""
    print(f"ğŸ”¢ ç”Ÿæˆå‘é‡ (æ¨¡å‹: {EMBEDDING_MODEL})")
    print("  é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 80MBï¼‰ï¼Œä¹‹åä¼šä½¿ç”¨ç¼“å­˜")
    
    # åŠ è½½æœ¬åœ°æ¨¡å‹
    model = SentenceTransformer(EMBEDDING_MODEL)
    
    # æ‰¹é‡å¤„ç†
    batch_size = 32
    all_embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        print(f"  å¤„ç†è¿›åº¦: {i}/{len(texts)}")
        
        # ç”Ÿæˆå‘é‡
        embeddings = model.encode(batch, show_progress_bar=False)
        all_embeddings.extend(embeddings.tolist())
    
    print(f"âœ“ ç”Ÿæˆäº† {len(all_embeddings)} ä¸ªå‘é‡")
    return all_embeddings


def save_vectors(chunks: List[Dict], embeddings: List[List[float]], output_path: Path):
    """ä¿å­˜ä¸ºå•ä¸ª JSON æ–‡ä»¶"""
    print(f"ğŸ’¾ ä¿å­˜å‘é‡æ•°æ®: {output_path}")
    
    # ç»„åˆæ•°æ®
    data = {
        'metadata': {
            'total_chunks': len(chunks),
            'embedding_model': EMBEDDING_MODEL,
            'embedding_dim': len(embeddings[0]) if embeddings else 0,
            'chunk_size': CHUNK_SIZE,
            'chunk_overlap': CHUNK_OVERLAP
        },
        'chunks': []
    }
    
    for chunk, embedding in zip(chunks, embeddings):
        data['chunks'].append({
            'id': chunk['id'],
            'page': chunk['page'],
            'text': chunk['text'],
            'embedding': embedding
        })
    
    # ä¿å­˜
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
    size_mb = output_path.stat().st_size / (1024 * 1024)
    print(f"âœ“ ä¿å­˜æˆåŠŸï¼æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")


def main():
    """ä¸»æµç¨‹"""
    print("=" * 60)
    print("å‘é‡åŒ– Sutton RL æ•™ç§‘ä¹¦")
    print("=" * 60)
    
    # 1. æå–æ–‡æœ¬
    pages = extract_text_from_pdf(PDF_PATH)
    
    # 2. åˆ†å—
    chunks = chunk_text(pages, CHUNK_SIZE, CHUNK_OVERLAP)
    
    # 3. ç”Ÿæˆå‘é‡
    texts = [chunk['text'] for chunk in chunks]
    embeddings = get_embeddings(texts)
    
    # 4. ä¿å­˜
    save_vectors(chunks, embeddings, OUTPUT_PATH)
    
    print("\n" + "=" * 60)
    print("âœ… å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {OUTPUT_PATH}")
    print(f"ğŸ“Š æ€»å—æ•°: {len(chunks)}")
    print("=" * 60)


if __name__ == "__main__":
    main()
