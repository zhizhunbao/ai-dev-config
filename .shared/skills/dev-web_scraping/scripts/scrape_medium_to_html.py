#!/usr/bin/env python3
"""
æŠ“å– Medium æ–‡ç« å¹¶è½¬æ¢ä¸º Markdown

åŠŸèƒ½ï¼š
- ä½¿ç”¨ Playwright åŠ è½½å®Œæ•´é¡µé¢ï¼ˆåŒ…æ‹¬åŠ¨æ€å†…å®¹ï¼‰
- ä¸‹è½½å®Œæ•´ HTML åˆ°æœ¬åœ°
- ä½¿ç”¨ html2text è½¬æ¢ä¸º Markdown
- è‡ªåŠ¨æ¸…ç† UI å…ƒç´ å’Œå¹¿å‘Šå†…å®¹
- ä¿ç•™æ–‡ç« ç»“æ„å’Œå›¾ç‰‡é“¾æ¥

ç”¨æ³•:
    uv run python scrape_medium_to_html.py <url> [-o output.md] [--keep-html]

ç¤ºä¾‹:
    uv run python scrape_medium_to_html.py https://medium.com/@author/article
    uv run python scrape_medium_to_html.py https://medium.com/@author/article -o article.md
    uv run python scrape_medium_to_html.py https://medium.com/@author/article --keep-html
"""

import asyncio
import sys
import argparse
from pathlib import Path
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout
import html2text
import re
import httpx
from bs4 import BeautifulSoup


class MediumScraper:
    """Medium æ–‡ç« æŠ“å–å™¨"""
    
    def __init__(self, url: str):
        self.url = url
        self.updated_url = None
    
    async def check_for_updates(self, page) -> str:
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦æœ‰æ›´æ–°ç‰ˆæœ¬é“¾æ¥"""
        try:
            # æŸ¥æ‰¾æ›´æ–°é“¾æ¥ï¼ˆé€šå¸¸åœ¨æ–‡ç« å¼€å¤´ï¼‰
            update_links = await page.evaluate("""
                () => {
                    const text = document.body.innerText;
                    const updateMatch = text.match(/UPDATE.*?(https?:\/\/[^\s\)]+)/i);
                    return updateMatch ? updateMatch[1] : null;
                }
            """)
            
            if update_links:
                print(f"  â„¹ï¸  æ£€æµ‹åˆ°æ›´æ–°ç‰ˆæœ¬: {update_links}")
                return update_links
            
            return None
        except Exception as e:
            print(f"  âš ï¸  æ£€æŸ¥æ›´æ–°å¤±è´¥: {e}")
            return None
    
    async def save_html(self, html_path: Path):
        """ä¿å­˜å®Œæ•´çš„ HTML é¡µé¢"""
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            
            page = await context.new_page()
            
            try:
                print(f"ğŸ“„ æ­£åœ¨è®¿é—®: {self.url}")
                await page.goto(self.url, wait_until='domcontentloaded', timeout=60000)
                
                # ç­‰å¾…æ–‡ç« åŠ è½½
                try:
                    await page.wait_for_selector('article', timeout=15000)
                    print("âœ“ æ–‡ç« å†…å®¹å·²åŠ è½½")
                except:
                    print("âš ï¸  ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­...")
                
                # å…³é—­å¼¹çª—
                await page.keyboard.press('Escape')
                await page.wait_for_timeout(1000)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ›´æ–°ç‰ˆæœ¬
                self.updated_url = await self.check_for_updates(page)
                
                # æ»šåŠ¨åŠ è½½
                await page.evaluate("""
                    async () => {
                        await new Promise((resolve) => {
                            let totalHeight = 0;
                            const distance = 500;
                            const timer = setInterval(() => {
                                window.scrollBy(0, distance);
                                totalHeight += distance;
                                if(totalHeight >= document.documentElement.scrollHeight){
                                    clearInterval(timer);
                                    resolve();
                                }
                            }, 100);
                        });
                    }
                """)
                await page.wait_for_timeout(2000)
                print("  âœ“ æ‰€æœ‰å†…å®¹å·²åŠ è½½")
                
                # ç­‰å¾… Gist åµŒå…¥åŠ è½½ï¼ˆMedium ä½¿ç”¨ iframe åµŒå…¥ä»£ç ï¼‰
                try:
                    await page.wait_for_selector('iframe[src*="gist.github.com"]', timeout=5000)
                    print("  âœ“ æ£€æµ‹åˆ° GitHub Gist ä»£ç å—")
                    await page.wait_for_timeout(2000)  # ç­‰å¾… iframe å†…å®¹åŠ è½½
                except:
                    print("  â„¹ï¸  æœªæ£€æµ‹åˆ° Gist ä»£ç å—")
                
                # è·å– HTML
                html_content = await page.content()
                html_path.write_text(html_content, encoding='utf-8')
                print(f"âœ“ HTML å·²ä¿å­˜: {html_path}")
                
                # æå– Gist URLs
                gist_urls = await page.evaluate("""
                    () => {
                        const iframes = document.querySelectorAll('iframe[src*="gist.github.com"]');
                        return Array.from(iframes).map(iframe => iframe.src);
                    }
                """)
                
                await browser.close()
                return gist_urls
                
            except Exception as e:
                print(f"âœ— å¤±è´¥: {e}")
                await browser.close()
                return []
    
    def html_to_markdown(self, html_path: Path, md_path: Path, gist_urls: list):
        """å°† HTML è½¬æ¢ä¸º Markdown"""
        
        print(f"\nğŸ“ æ­£åœ¨è½¬æ¢ä¸º Markdown...")
        
        html_content = html_path.read_text(encoding='utf-8')
        
        # è½¬æ¢
        h = html2text.HTML2Text()
        h.ignore_links = False
        h.ignore_images = False
        h.body_width = 0
        h.unicode_snob = True
        
        markdown = h.handle(html_content)
        
        # æ¸…ç†
        markdown = self._clean_markdown(markdown)
        
        # æ·»åŠ  Gist ä»£ç 
        if gist_urls:
            print(f"  â„¹ï¸  æ£€æµ‹åˆ° {len(gist_urls)} ä¸ª Gist ä»£ç å—")
            markdown = self._append_gist_code(markdown, gist_urls)
        
        # å¦‚æœæœ‰æ›´æ–°ç‰ˆæœ¬ï¼Œæ·»åŠ æç¤ºå¹¶å°è¯•è·å–ä»£ç 
        if self.updated_url:
            print(f"  â„¹ï¸  å°è¯•ä»æ›´æ–°ç‰ˆæœ¬è·å–ä»£ç : {self.updated_url}")
            markdown = self._append_updated_content(markdown)
        
        md_path.write_text(markdown, encoding='utf-8')
        print(f"âœ“ Markdown å·²ä¿å­˜: {md_path}")
    
    def _clean_markdown(self, markdown: str) -> str:
        """æ¸…ç† Markdown - ç§»é™¤ UI å…ƒç´ """
        
        lines = markdown.split('\n')
        cleaned = []
        
        skip_keywords = [
            'Sign up', 'Sign in', 'Follow', 'Share', 'Save', 'Listen',
            'Open in app', 'Member-only', 'Clap', 'Response',
            'More from', 'Written by', 'Help', 'Status', 'About',
            'Careers', 'Press', 'Blog', 'Privacy', 'Terms',
            'Join Medium', 'Create account', 'Subscribe',
            'TDS Archive', 'publication', 'min read',
            'Write', '/m/signin', 'Follow publication',
            'An archive of', 'Get updates', 'stories in your inbox'
        ]
        
        end_markers = [
            '[Machine Learning](/tag/',
            '[Reinforcement Learning](/tag/',
            'See all from',
            'Recommended from Medium'
        ]
        
        article_ended = False
        article_started = False
        
        for line in lines:
            if article_ended:
                break
            
            if any(m in line for m in end_markers):
                article_ended = True
                continue
            
            if any(k in line for k in skip_keywords):
                continue
            
            stripped = line.strip()
            
            # è·³è¿‡ç©ºå†…å®¹
            if stripped in ['[]()', '[]', '##', '#', 'Â·', '']:
                continue
            
            # è·³è¿‡å°å›¾æ ‡
            if stripped.startswith('![](') and 'resize:fill:' in stripped:
                if any(s in stripped for s in [':32:', ':38:', ':48:', ':64:']):
                    continue
            
            # è·³è¿‡çº¯æ•°å­—
            if stripped.isdigit() and len(stripped) < 5:
                continue
            
            # è·³è¿‡å¯¼èˆªé“¾æ¥
            if stripped.startswith('[') and '/m/signin' in stripped:
                continue
            
            # æ£€æµ‹æ–‡ç« å¼€å§‹
            if not article_started:
                if stripped.startswith('# ') or stripped.startswith('## '):
                    title = stripped.lstrip('#').strip()
                    if title and len(title) > 5:
                        article_started = True
            
            if article_started:
                cleaned.append(line)
        
        # æ·»åŠ å¤´éƒ¨
        header = f"# Q-Learning Math - Python Code\n\n"
        header += f"**Source:** {self.url}\n\n"
        header += "---\n\n"
        
        return header + '\n'.join(cleaned)
    
    def _append_gist_code(self, markdown: str, gist_urls: list) -> str:
        """ä» Gist URLs è·å–ä»£ç å¹¶æ·»åŠ åˆ° Markdown"""
        
        code_section = "\n\n---\n\n## ğŸ“¦ Code from Article\n\n"
        code_section += "> The following code blocks were embedded in the original article via GitHub Gist.\n\n"
        
        for i, gist_url in enumerate(gist_urls, 1):
            try:
                # ä» iframe URL æå– Gist ID
                # æ ¼å¼: https://medium.com/media/{hash}/href?url=https://gist.github.com/{user}/{gist_id}
                match = re.search(r'gist\.github\.com/([^/]+)/([^/?]+)', gist_url)
                if not match:
                    print(f"    âš ï¸  æ— æ³•è§£æ Gist URL: {gist_url}")
                    continue
                
                user, gist_id = match.groups()
                raw_url = f"https://gist.githubusercontent.com/{user}/{gist_id}/raw/"
                
                print(f"    æ­£åœ¨è·å– Gist {i}/{len(gist_urls)}: {user}/{gist_id}")
                
                # è·å– Gist å†…å®¹
                response = httpx.get(raw_url, follow_redirects=True, timeout=10)
                if response.status_code == 200:
                    code_content = response.text
                    
                    # æ£€æµ‹è¯­è¨€ï¼ˆä»æ–‡ä»¶æ‰©å±•åæˆ–å†…å®¹ï¼‰
                    language = "python"  # é»˜è®¤
                    if ".py" in gist_url or "python" in code_content.lower()[:100]:
                        language = "python"
                    elif ".js" in gist_url:
                        language = "javascript"
                    
                    code_section += f"### Code Block {i}\n\n"
                    code_section += f"**Source:** [{user}/{gist_id}](https://gist.github.com/{user}/{gist_id})\n\n"
                    code_section += f"```{language}\n{code_content}\n```\n\n"
                    print(f"    âœ“ å·²è·å–ä»£ç å— {i}")
                else:
                    print(f"    âš ï¸  è·å–å¤±è´¥ (HTTP {response.status_code})")
                    code_section += f"### Code Block {i}\n\n"
                    code_section += f"**Source:** [View on GitHub](https://gist.github.com/{user}/{gist_id})\n\n"
                    code_section += f"> âš ï¸ Failed to fetch code automatically. Please visit the link above.\n\n"
                    
            except Exception as e:
                print(f"    âœ— è·å– Gist å¤±è´¥: {e}")
                code_section += f"### Code Block {i}\n\n"
                code_section += f"> âš ï¸ Error fetching code: {e}\n\n"
        
        return markdown + code_section
    
    def _append_updated_content(self, markdown: str) -> str:
        """ä»æ›´æ–°ç‰ˆæœ¬è·å–ä»£ç """
        
        if not self.updated_url:
            return markdown
        
        try:
            print(f"    æ­£åœ¨è·å–æ›´æ–°ç‰ˆæœ¬å†…å®¹...")
            response = httpx.get(self.updated_url, follow_redirects=True, timeout=30)
            
            if response.status_code != 200:
                print(f"    âš ï¸  è·å–å¤±è´¥ (HTTP {response.status_code})")
                return markdown + f"\n\n---\n\n## ğŸ“Œ Updated Version\n\n**URL:** {self.updated_url}\n\n> âš ï¸ Failed to fetch updated content. Please visit the link above.\n\n"
            
            from bs4 import BeautifulSoup
            
            # ä½¿ç”¨ BeautifulSoup è§£æ HTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰ä»£ç å— (pre > code)
            code_blocks = soup.find_all('pre')
            
            if code_blocks:
                print(f"    âœ“ ä»æ›´æ–°ç‰ˆæœ¬æå–åˆ° {len(code_blocks)} ä¸ªä»£ç å—")
                
                code_section = "\n\n---\n\n## ğŸ“¦ Code from Updated Version\n\n"
                code_section += f"**Source:** [{self.updated_url}]({self.updated_url})\n\n"
                code_section += "> The following code is from the updated version of the article.\n\n"
                
                for i, pre in enumerate(code_blocks, 1):
                    code = pre.get_text()
                    
                    # è·³è¿‡å¤ªçŸ­çš„ä»£ç å—ï¼ˆå¯èƒ½æ˜¯å†…è”ä»£ç ï¼‰
                    if len(code.strip()) < 20:
                        continue
                    
                    # æ£€æµ‹è¯­è¨€
                    language = "python"  # é»˜è®¤
                    if "class" in code or "def" in code or "import" in code:
                        language = "python"
                    
                    code_section += f"### Code Block {i}\n\n```{language}\n{code.strip()}\n```\n\n"
                
                return markdown + code_section
            else:
                print(f"    âš ï¸  æœªæ‰¾åˆ°ä»£ç å—")
                return markdown + f"\n\n---\n\n## ğŸ“Œ Updated Version\n\n**URL:** [{self.updated_url}]({self.updated_url})\n\n> Please visit the updated version for complete code examples.\n\n"
                
        except Exception as e:
            print(f"    âœ— è·å–æ›´æ–°ç‰ˆæœ¬å¤±è´¥: {e}")
            return markdown + f"\n\n---\n\n## ğŸ“Œ Updated Version\n\n**URL:** {self.updated_url}\n\n> âš ï¸ Error fetching updated content: {e}\n\n"


async def main():
    parser = argparse.ArgumentParser(description="æŠ“å– Medium æ–‡ç« ")
    parser.add_argument("url", help="æ–‡ç«  URL")
    parser.add_argument("-o", "--output", type=Path, help="è¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--keep-html", action="store_true", help="ä¿ç•™ HTML")
    
    args = parser.parse_args()
    
    if not args.output:
        slug = args.url.rstrip('/').split('/')[-1]
        args.output = Path(f"{slug}.md")
    
    html_path = args.output.with_suffix('.html')
    
    scraper = MediumScraper(args.url)
    
    gist_urls = await scraper.save_html(html_path)
    if gist_urls is None:
        sys.exit(1)
    
    scraper.html_to_markdown(html_path, args.output, gist_urls)
    
    if not args.keep_html:
        html_path.unlink()
        print(f"  å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {html_path}")
    
    print(f"\nâœ… å®Œæˆ! è¾“å‡º: {args.output}")


if __name__ == '__main__':
    asyncio.run(main())
